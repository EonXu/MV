{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, concatenate, GlobalAveragePooling2D, Dense, MaxPooling2D, Dropout, Flatten, ReLU, Add, BatchNormalization, AveragePooling2D, Multiply, GlobalAveragePooling2D, Reshape,DepthwiseConv2D\n",
    "from keras.optimizers import RMSprop\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from PIL import Image\n",
    "from tensorflow.keras.metrics import Metric,Precision, Recall\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3, MobileNet\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'E:\\MV\\coursework\\fruits-360\\Training'\n",
    "test_dir = r'E:\\MV\\coursework\\fruits-360\\Test'\n",
    "#The total number of images: 90483.\n",
    "#Training set size: 67692 images (one fruit or vegetable per image).\n",
    "#Test set size: 22688 images (one fruit or vegetable per image).\n",
    "#The number of classes: 131 (fruits and vegetables).\n",
    "#Image size: 100x100 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumberOfClass:  131\n"
     ]
    }
   ],
   "source": [
    "#find out the total calsses of dataset.\n",
    "className = glob.glob(train_dir + \"/*\")\n",
    "numberOfClass = len(className)\n",
    "print(\"NumberOfClass: \",numberOfClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_background(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY)\n",
    "    thresh_rgb = cv2.cvtColor(thresh, cv2.COLOR_GRAY2RGB)\n",
    "    np.copyto(image, thresh_rgb, where=thresh_rgb == 255)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 63145 images belonging to 131 classes.\n",
      "Found 4547 images belonging to 131 classes.\n",
      "Found 22688 images belonging to 131 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=denoise_background,\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.068,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(100, 100),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb', \n",
    "    subset='training' \n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(100,100),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb', \n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Load and preprocess the test data\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(100,100),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = Input(shape=(100,100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAccuracy(Metric):\n",
    "    def __init__(self, name='accuracy', **kwargs):\n",
    "        super(CustomAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.correct = self.add_weight(name='correct', initializer='zeros')\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_classes = tf.argmax(y_pred, axis=1)\n",
    "        y_true_classes = tf.argmax(y_true, axis=1)\n",
    "        correct_predictions = tf.equal(y_pred_classes, y_true_classes)\n",
    "        correct_predictions = tf.cast(correct_predictions, 'float32')\n",
    "        \n",
    "        self.correct.assign_add(tf.reduce_sum(correct_predictions))\n",
    "        self.total.assign_add(tf.cast(tf.size(y_true_classes), 'float32'))\n",
    "\n",
    "    def result(self):\n",
    "        return tf.divide(self.correct, self.total)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.correct.assign(0)\n",
    "        self.total.assign(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCrossEntropyLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, name='loss', **kwargs):\n",
    "        super(CustomCrossEntropyLoss, self).__init__(name=name, **kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        epsilon = 1e-10\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "        loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: 3 inception block +SE  \n",
    "\n",
    "Model2: 4 res_block+DepthwiseConv+SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_block(input_tensor, ratio=16):\n",
    "# Squeeze-and-Excitation block\n",
    "    channels = input_tensor.shape[-1]\n",
    "    se = GlobalAveragePooling2D()(input_tensor)\n",
    "    se = Dense(channels // ratio, activation='relu')(se)\n",
    "    se = Dense(channels, activation='sigmoid')(se)\n",
    "    se = Reshape((1, 1, channels))(se)\n",
    "    scaled_input = Multiply()([input_tensor, se])\n",
    "    return scaled_input\n",
    "\n",
    "def inception_block(x, br1, br2_1, br2_2, br3_1, br3_2, br4):\n",
    "    # Branch 1\n",
    "    branch1 = Conv2D(br1, (1, 1), padding='same', activation='relu')(x)\n",
    "    # Branch 2 3x3-> 1x3 3x1\n",
    "    branch2 = Conv2D(br2_1, (1, 1), padding='same', activation='relu')(x)\n",
    "    #branch2 = Conv2D(br2_2, (3, 3), padding='same', activation='relu')(branch2)\n",
    "    branch2 = Conv2D(br2_2, (1, 3), padding='same', activation='relu')(branch2)\n",
    "    branch2 = Conv2D(br2_2, (3, 1), padding='same', activation='relu')(branch2)\n",
    "    # Branch 3 5x5->3x3 3x3-> 1x3 3x1 1x3 3x1 \n",
    "    branch3 = Conv2D(br3_1, (1, 1), padding='same', activation='relu')(x)\n",
    "    #branch3 = Conv2D(br3_2, (5, 5), padding='same', activation='relu')(branch3)\n",
    "    branch3 = Conv2D(br3_2, (1, 3), padding='same', activation='relu')(branch3)\n",
    "    branch3 = Conv2D(br3_2, (3, 1), padding='same', activation='relu')(branch3)\n",
    "    branch3 = Conv2D(br3_2, (1, 3), padding='same', activation='relu')(branch3)\n",
    "    branch3 = Conv2D(br3_2, (3, 1), padding='same', activation='relu')(branch3)\n",
    "    # Branch 4\n",
    "    branch4 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch4 = Conv2D(br4, (1, 1), padding='same', activation='relu')(branch4)\n",
    "    # Concatenate all the branches\n",
    "    branch_output = concatenate([branch1, branch2, branch3, branch4], axis=-1)\n",
    "    # After the Inception block, apply the SE block\n",
    "    branch_output = se_block(branch_output)\n",
    "    return branch_output\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_shape)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(2, 2)(x)\n",
    "x = inception_block(x, 64, 96, 64, 16, 32, 32)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(2, 2)(x)\n",
    "x = inception_block(x, 128, 128, 128, 32, 64, 64)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(2, 2)(x)\n",
    "x = inception_block(x, 160, 160, 160, 48, 96, 96)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(2, 2)(x)\n",
    "# Final layers\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "output = Dense(numberOfClass, activation='softmax')(x)\n",
    "model_1 = Model(inputs=input_shape, outputs=output)\n",
    "\n",
    "\n",
    "def res_block(input_tensor, filters, strides=1):\n",
    "    shortcut = input_tensor\n",
    "\n",
    "    # First component of main path\n",
    "    x = Conv2D(filters, (1, 1), strides=strides, padding='same')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Second component of main path\n",
    "    x = DepthwiseConv2D((3, 3), padding='same', depth_multiplier=1)(x)\n",
    "    x = Conv2D(filters, (1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Third component of main path\n",
    "    x = Conv2D(filters * 4, (1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = se_block(x)\n",
    "\n",
    "    # Shortcut path\n",
    "    if strides != 1 or shortcut.shape[-1] != filters * 4:\n",
    "        shortcut = Conv2D(filters * 4, (1, 1), strides=strides, padding='same')(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    # Final step: Add shortcut value to main path\n",
    "    x = Add()([x, shortcut])\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def combined_model():\n",
    "\n",
    "    x = Conv2D(64, kernel_size=(7, 7), strides=2, padding='same')(input_shape)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D(2,2)(x)\n",
    "\n",
    "    x = res_block(x, filters=64, strides=1)\n",
    "    x = res_block(x, filters=128, strides=2)\n",
    "    x = res_block(x, filters=256, strides=2)\n",
    "    x = res_block(x, filters=512, strides=2)\n",
    "    x = MaxPooling2D(2,2)(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    output = Dense(numberOfClass, activation='softmax')(x)\n",
    "\n",
    "    model_2 = Model(inputs=input_shape, outputs=output)\n",
    "    concatenated_output= concatenate([model_1.output, model_2.output])\n",
    "\n",
    "    x = Dense(128, activation='relu')(concatenated_output) \n",
    "    x = BatchNormalization()(x) \n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x) \n",
    "    x = Dropout(0.3)(x)\n",
    "    ensemble_output= Dense(numberOfClass, activation='softmax')(x)\n",
    "    #Build the ensemble model\n",
    "    combined_model = Model(inputs=input_shape, outputs= ensemble_output)\n",
    "    return combined_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the output layer for the other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_last_layer(base_model):\n",
    "    x = Dense(128, activation='relu')(base_model.output) \n",
    "    x = BatchNormalization()(x) \n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x) \n",
    "    x = Dropout(0.3)(x)\n",
    "    last_output= Dense(numberOfClass, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=last_output)\n",
    "    return model \n",
    "\n",
    "def prepare_model(model):\n",
    "    model = add_new_last_layer(model)\n",
    "    model.compile(optimizer=RMSprop(learning_rate=0.001), loss=CustomCrossEntropyLoss(),metrics=[CustomAccuracy(), Precision(name='precision'),Recall(name='recall')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv3_model = prepare_model(InceptionV3(weights=None, include_top=False, input_shape=(100,100,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the inceptionv3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1974/1974 [==============================] - 251s 119ms/step - loss: 2.4560 - accuracy: 0.2917 - precision: 0.5810 - recall: 0.1189 - val_loss: 3.6593 - val_accuracy: 0.3160 - val_precision: 0.3605 - val_recall: 0.2589\n",
      "Epoch 2/40\n",
      "1974/1974 [==============================] - 226s 114ms/step - loss: 1.1630 - accuracy: 0.6210 - precision: 0.7186 - recall: 0.5170 - val_loss: 1.0115 - val_accuracy: 0.7431 - val_precision: 0.8056 - val_recall: 0.6901\n",
      "Epoch 3/40\n",
      "1974/1974 [==============================] - 225s 114ms/step - loss: 0.7692 - accuracy: 0.7492 - precision: 0.7969 - recall: 0.7035 - val_loss: 0.7652 - val_accuracy: 0.7829 - val_precision: 0.8053 - val_recall: 0.7561\n",
      "Epoch 4/40\n",
      "1974/1974 [==============================] - 224s 113ms/step - loss: 0.5930 - accuracy: 0.8112 - precision: 0.8405 - recall: 0.7861 - val_loss: 1.1997 - val_accuracy: 0.7211 - val_precision: 0.7541 - val_recall: 0.6980\n",
      "Epoch 5/40\n",
      "1974/1974 [==============================] - 221s 112ms/step - loss: 0.4649 - accuracy: 0.8576 - precision: 0.8753 - recall: 0.8418 - val_loss: 1.2282 - val_accuracy: 0.8280 - val_precision: 0.8389 - val_recall: 0.8179\n",
      "Epoch 6/40\n",
      "1974/1974 [==============================] - 224s 113ms/step - loss: 0.4021 - accuracy: 0.8791 - precision: 0.8926 - recall: 0.8680 - val_loss: 0.4733 - val_accuracy: 0.8953 - val_precision: 0.9026 - val_recall: 0.8887\n",
      "Epoch 7/40\n",
      "1974/1974 [==============================] - 227s 115ms/step - loss: 0.3551 - accuracy: 0.8969 - precision: 0.9075 - recall: 0.8880 - val_loss: 0.4667 - val_accuracy: 0.8531 - val_precision: 0.8652 - val_recall: 0.8430\n",
      "Epoch 8/40\n",
      "1974/1974 [==============================] - 225s 114ms/step - loss: 0.3324 - accuracy: 0.9030 - precision: 0.9122 - recall: 0.8952 - val_loss: 0.8135 - val_accuracy: 0.8788 - val_precision: 0.8899 - val_recall: 0.8678\n",
      "Epoch 9/40\n",
      "1974/1974 [==============================] - 223s 113ms/step - loss: 0.3046 - accuracy: 0.9126 - precision: 0.9202 - recall: 0.9068 - val_loss: 0.4748 - val_accuracy: 0.9063 - val_precision: 0.9177 - val_recall: 0.9006\n",
      "Epoch 10/40\n",
      "1974/1974 [==============================] - 222s 112ms/step - loss: 0.2853 - accuracy: 0.9221 - precision: 0.9285 - recall: 0.9163 - val_loss: 0.3820 - val_accuracy: 0.9147 - val_precision: 0.9231 - val_recall: 0.9059\n",
      "Epoch 11/40\n",
      "1974/1974 [==============================] - 223s 113ms/step - loss: 0.2714 - accuracy: 0.9243 - precision: 0.9309 - recall: 0.9193 - val_loss: 1.0991 - val_accuracy: 0.8386 - val_precision: 0.8438 - val_recall: 0.8337\n",
      "Epoch 12/40\n",
      "1974/1974 [==============================] - 225s 114ms/step - loss: 0.2677 - accuracy: 0.9281 - precision: 0.9336 - recall: 0.9235 - val_loss: 0.5204 - val_accuracy: 0.9252 - val_precision: 0.9305 - val_recall: 0.9193\n",
      "Epoch 13/40\n",
      "1974/1974 [==============================] - 225s 114ms/step - loss: 0.2404 - accuracy: 0.9356 - precision: 0.9400 - recall: 0.9318 - val_loss: 0.6377 - val_accuracy: 0.9092 - val_precision: 0.9151 - val_recall: 0.9061\n",
      "Epoch 14/40\n",
      "1974/1974 [==============================] - 225s 114ms/step - loss: 0.2313 - accuracy: 0.9388 - precision: 0.9432 - recall: 0.9357 - val_loss: 0.4900 - val_accuracy: 0.9408 - val_precision: 0.9455 - val_recall: 0.9351\n",
      "Epoch 15/40\n",
      "1974/1974 [==============================] - 223s 113ms/step - loss: 0.2287 - accuracy: 0.9418 - precision: 0.9456 - recall: 0.9389 - val_loss: 0.6624 - val_accuracy: 0.9477 - val_precision: 0.9505 - val_recall: 0.9468\n",
      "Epoch 16/40\n",
      "1974/1974 [==============================] - 227s 115ms/step - loss: 0.2078 - accuracy: 0.9457 - precision: 0.9491 - recall: 0.9426 - val_loss: 0.9473 - val_accuracy: 0.8401 - val_precision: 0.8491 - val_recall: 0.8318\n",
      "Epoch 17/40\n",
      "1974/1974 [==============================] - 228s 115ms/step - loss: 0.2159 - accuracy: 0.9456 - precision: 0.9492 - recall: 0.9430 - val_loss: 0.8914 - val_accuracy: 0.9133 - val_precision: 0.9201 - val_recall: 0.9094\n",
      "Epoch 18/40\n",
      "1974/1974 [==============================] - 225s 114ms/step - loss: 0.2274 - accuracy: 0.9451 - precision: 0.9485 - recall: 0.9424 - val_loss: 0.6116 - val_accuracy: 0.9327 - val_precision: 0.9342 - val_recall: 0.9307\n",
      "Epoch 19/40\n",
      "1974/1974 [==============================] - 688s 349ms/step - loss: 0.2015 - accuracy: 0.9505 - precision: 0.9534 - recall: 0.9482 - val_loss: 2.0804 - val_accuracy: 0.7933 - val_precision: 0.7988 - val_recall: 0.7882\n",
      "Epoch 20/40\n",
      "1974/1974 [==============================] - 221s 112ms/step - loss: 0.1962 - accuracy: 0.9547 - precision: 0.9571 - recall: 0.9526 - val_loss: 0.8750 - val_accuracy: 0.9283 - val_precision: 0.9312 - val_recall: 0.9232\n",
      "Epoch 21/40\n",
      "1974/1974 [==============================] - 223s 113ms/step - loss: 0.2011 - accuracy: 0.9538 - precision: 0.9563 - recall: 0.9521 - val_loss: 0.3243 - val_accuracy: 0.9446 - val_precision: 0.9461 - val_recall: 0.9419\n",
      "Epoch 22/40\n",
      "1974/1974 [==============================] - 226s 114ms/step - loss: 0.2140 - accuracy: 0.9547 - precision: 0.9572 - recall: 0.9531 - val_loss: 0.3402 - val_accuracy: 0.9235 - val_precision: 0.9284 - val_recall: 0.9186\n",
      "Epoch 23/40\n",
      "1974/1974 [==============================] - 224s 113ms/step - loss: 0.1957 - accuracy: 0.9563 - precision: 0.9586 - recall: 0.9546 - val_loss: 0.8126 - val_accuracy: 0.8663 - val_precision: 0.8776 - val_recall: 0.8590\n",
      "Epoch 24/40\n",
      "1974/1974 [==============================] - 227s 115ms/step - loss: 0.1860 - accuracy: 0.9577 - precision: 0.9597 - recall: 0.9562 - val_loss: 0.4801 - val_accuracy: 0.9562 - val_precision: 0.9570 - val_recall: 0.9549\n",
      "Epoch 25/40\n",
      "1974/1974 [==============================] - 224s 113ms/step - loss: 0.1919 - accuracy: 0.9590 - precision: 0.9611 - recall: 0.9571 - val_loss: 0.5833 - val_accuracy: 0.9329 - val_precision: 0.9357 - val_recall: 0.9307\n",
      "Epoch 26/40\n",
      "1974/1974 [==============================] - 223s 113ms/step - loss: 0.1808 - accuracy: 0.9600 - precision: 0.9620 - recall: 0.9587 - val_loss: 0.6898 - val_accuracy: 0.9461 - val_precision: 0.9470 - val_recall: 0.9461\n",
      "Epoch 27/40\n",
      "1974/1974 [==============================] - 222s 113ms/step - loss: 0.1866 - accuracy: 0.9590 - precision: 0.9609 - recall: 0.9577 - val_loss: 0.1139 - val_accuracy: 0.9688 - val_precision: 0.9722 - val_recall: 0.9677\n",
      "Epoch 28/40\n",
      "1974/1974 [==============================] - 223s 113ms/step - loss: 0.1746 - accuracy: 0.9612 - precision: 0.9628 - recall: 0.9599 - val_loss: 0.2093 - val_accuracy: 0.9670 - val_precision: 0.9680 - val_recall: 0.9659\n",
      "Epoch 29/40\n",
      "1974/1974 [==============================] - 225s 114ms/step - loss: 0.1807 - accuracy: 0.9631 - precision: 0.9650 - recall: 0.9616 - val_loss: 1.4665 - val_accuracy: 0.8256 - val_precision: 0.8347 - val_recall: 0.8205\n",
      "Epoch 30/40\n",
      "1974/1974 [==============================] - 226s 115ms/step - loss: 0.1835 - accuracy: 0.9629 - precision: 0.9646 - recall: 0.9615 - val_loss: 1.1566 - val_accuracy: 0.9085 - val_precision: 0.9097 - val_recall: 0.9063\n",
      "Epoch 31/40\n",
      "1974/1974 [==============================] - 226s 115ms/step - loss: 0.1758 - accuracy: 0.9650 - precision: 0.9664 - recall: 0.9638 - val_loss: 0.3753 - val_accuracy: 0.9617 - val_precision: 0.9631 - val_recall: 0.9598\n",
      "Epoch 32/40\n",
      "1974/1974 [==============================] - 224s 113ms/step - loss: 0.1585 - accuracy: 0.9667 - precision: 0.9680 - recall: 0.9658 - val_loss: 0.6354 - val_accuracy: 0.9439 - val_precision: 0.9457 - val_recall: 0.9424\n",
      "Epoch 33/40\n",
      "1974/1974 [==============================] - 225s 114ms/step - loss: 0.1524 - accuracy: 0.9677 - precision: 0.9690 - recall: 0.9667 - val_loss: 0.3904 - val_accuracy: 0.9549 - val_precision: 0.9565 - val_recall: 0.9527\n",
      "Epoch 34/40\n",
      "1974/1974 [==============================] - 225s 114ms/step - loss: 0.1527 - accuracy: 0.9677 - precision: 0.9689 - recall: 0.9666 - val_loss: 0.3549 - val_accuracy: 0.9672 - val_precision: 0.9680 - val_recall: 0.9659\n",
      "Epoch 35/40\n",
      "1974/1974 [==============================] - 226s 114ms/step - loss: 0.1735 - accuracy: 0.9654 - precision: 0.9667 - recall: 0.9644 - val_loss: 0.6254 - val_accuracy: 0.9358 - val_precision: 0.9387 - val_recall: 0.9327\n",
      "Epoch 36/40\n",
      "1974/1974 [==============================] - 225s 114ms/step - loss: 0.1604 - accuracy: 0.9671 - precision: 0.9684 - recall: 0.9658 - val_loss: 0.2861 - val_accuracy: 0.9703 - val_precision: 0.9709 - val_recall: 0.9690\n",
      "Epoch 37/40\n",
      "1974/1974 [==============================] - 223s 113ms/step - loss: 0.1712 - accuracy: 0.9676 - precision: 0.9688 - recall: 0.9667 - val_loss: 0.2250 - val_accuracy: 0.9793 - val_precision: 0.9793 - val_recall: 0.9791\n",
      "Epoch 38/40\n",
      "1974/1974 [==============================] - 227s 115ms/step - loss: 0.1805 - accuracy: 0.9672 - precision: 0.9685 - recall: 0.9663 - val_loss: 0.9077 - val_accuracy: 0.9008 - val_precision: 0.9071 - val_recall: 0.8955\n",
      "Epoch 39/40\n",
      "1974/1974 [==============================] - 226s 114ms/step - loss: 0.1603 - accuracy: 0.9688 - precision: 0.9699 - recall: 0.9679 - val_loss: 1.5854 - val_accuracy: 0.7374 - val_precision: 0.7853 - val_recall: 0.7247\n",
      "Epoch 40/40\n",
      "1974/1974 [==============================] - 227s 115ms/step - loss: 0.1722 - accuracy: 0.9684 - precision: 0.9696 - recall: 0.9673 - val_loss: 0.5887 - val_accuracy: 0.9400 - val_precision: 0.9421 - val_recall: 0.9375\n"
     ]
    }
   ],
   "source": [
    "history_inceptionv3=inceptionv3_model.fit(train_generator, epochs=40, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the history of inceptionv3\n",
    "save_history_inceptionv3 = pd.DataFrame(history_inceptionv3.history)\n",
    "\n",
    "save_history_inceptionv3.to_csv(\"history_inceptionv3_model_training_history_40epoch_1211.csv\", index=False)\n",
    "\n",
    "save_path=r'inceptionv3_model1213_40epochs.h5'\n",
    "inceptionv3_model.save(save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
