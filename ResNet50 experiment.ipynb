{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, concatenate, GlobalAveragePooling2D, Dense, MaxPooling2D, Dropout, Flatten, ReLU, Add, BatchNormalization, AveragePooling2D, Multiply, GlobalAveragePooling2D, Reshape,DepthwiseConv2D\n",
    "from keras.optimizers import RMSprop\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from PIL import Image\n",
    "from tensorflow.keras.metrics import Metric,Precision, Recall\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3, MobileNet\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'E:\\MV\\coursework\\fruits-360\\Training'\n",
    "test_dir = r'E:\\MV\\coursework\\fruits-360\\Test'\n",
    "#The total number of images: 90483.\n",
    "#Training set size: 67692 images (one fruit or vegetable per image).\n",
    "#Test set size: 22688 images (one fruit or vegetable per image).\n",
    "#The number of classes: 131 (fruits and vegetables).\n",
    "#Image size: 100x100 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumberOfClass:  131\n"
     ]
    }
   ],
   "source": [
    "#find out the total calsses of dataset.\n",
    "className = glob.glob(train_dir + \"/*\")\n",
    "numberOfClass = len(className)\n",
    "print(\"NumberOfClass: \",numberOfClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_background(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY)\n",
    "    thresh_rgb = cv2.cvtColor(thresh, cv2.COLOR_GRAY2RGB)\n",
    "    np.copyto(image, thresh_rgb, where=thresh_rgb == 255)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 63145 images belonging to 131 classes.\n",
      "Found 4547 images belonging to 131 classes.\n",
      "Found 22688 images belonging to 131 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=denoise_background,\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.068,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(100, 100),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb', \n",
    "    subset='training' \n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(100,100),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb', \n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Load and preprocess the test data\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(100,100),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = Input(shape=(100,100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAccuracy(Metric):\n",
    "    def __init__(self, name='accuracy', **kwargs):\n",
    "        super(CustomAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.correct = self.add_weight(name='correct', initializer='zeros')\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_classes = tf.argmax(y_pred, axis=1)\n",
    "        y_true_classes = tf.argmax(y_true, axis=1)\n",
    "        correct_predictions = tf.equal(y_pred_classes, y_true_classes)\n",
    "        correct_predictions = tf.cast(correct_predictions, 'float32')\n",
    "        \n",
    "        self.correct.assign_add(tf.reduce_sum(correct_predictions))\n",
    "        self.total.assign_add(tf.cast(tf.size(y_true_classes), 'float32'))\n",
    "\n",
    "    def result(self):\n",
    "        return tf.divide(self.correct, self.total)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.correct.assign(0)\n",
    "        self.total.assign(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCrossEntropyLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, name='loss', **kwargs):\n",
    "        super(CustomCrossEntropyLoss, self).__init__(name=name, **kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        epsilon = 1e-10\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "        loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the output layer for the other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_last_layer(base_model):\n",
    "    x = Dense(128, activation='relu')(base_model.output) \n",
    "    x = BatchNormalization()(x) \n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x) \n",
    "    x = Dropout(0.3)(x)\n",
    "    last_output= Dense(numberOfClass, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=last_output)\n",
    "    return model \n",
    "\n",
    "def prepare_model(model):\n",
    "    model = add_new_last_layer(model)\n",
    "    model.compile(optimizer=RMSprop(learning_rate=0.001), loss=CustomCrossEntropyLoss(),metrics=[CustomAccuracy(), Precision(name='precision'),Recall(name='recall')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_model = prepare_model(ResNet50(weights=None, include_top=False, input_shape=(100,100,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1974/1974 [==============================] - 519s 256ms/step - loss: 1.5713 - accuracy: 0.6242 - val_loss: 0.6700 - val_accuracy: 0.8276\n",
      "Epoch 2/40\n",
      "1974/1974 [==============================] - 337s 171ms/step - loss: 0.2249 - accuracy: 0.9405 - val_loss: 0.3250 - val_accuracy: 0.9197\n",
      "Epoch 3/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.1369 - accuracy: 0.9658 - val_loss: 0.2364 - val_accuracy: 0.9384\n",
      "Epoch 4/40\n",
      "1974/1974 [==============================] - 337s 171ms/step - loss: 0.1034 - accuracy: 0.9756 - val_loss: 4.3163 - val_accuracy: 0.4121\n",
      "Epoch 5/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0836 - accuracy: 0.9812 - val_loss: 0.4808 - val_accuracy: 0.9070\n",
      "Epoch 6/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0723 - accuracy: 0.9839 - val_loss: 0.5229 - val_accuracy: 0.9074\n",
      "Epoch 7/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0591 - accuracy: 0.9870 - val_loss: 0.1786 - val_accuracy: 0.9609\n",
      "Epoch 8/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0566 - accuracy: 0.9873 - val_loss: 0.5473 - val_accuracy: 0.9129\n",
      "Epoch 9/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0541 - accuracy: 0.9889 - val_loss: 0.2495 - val_accuracy: 0.9518\n",
      "Epoch 10/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0499 - accuracy: 0.9898 - val_loss: 0.2394 - val_accuracy: 0.9604\n",
      "Epoch 11/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0435 - accuracy: 0.9909 - val_loss: 0.0914 - val_accuracy: 0.9826\n",
      "Epoch 12/40\n",
      "1974/1974 [==============================] - 337s 171ms/step - loss: 0.0408 - accuracy: 0.9914 - val_loss: 0.1758 - val_accuracy: 0.9675\n",
      "Epoch 13/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0433 - accuracy: 0.9913 - val_loss: 0.1667 - val_accuracy: 0.9725\n",
      "Epoch 14/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0400 - accuracy: 0.9921 - val_loss: 0.2897 - val_accuracy: 0.9532\n",
      "Epoch 15/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0346 - accuracy: 0.9930 - val_loss: 0.2799 - val_accuracy: 0.9637\n",
      "Epoch 16/40\n",
      "1974/1974 [==============================] - 337s 170ms/step - loss: 0.0386 - accuracy: 0.9927 - val_loss: 0.0544 - val_accuracy: 0.9883\n",
      "Epoch 17/40\n",
      "1974/1974 [==============================] - 337s 171ms/step - loss: 0.0379 - accuracy: 0.9928 - val_loss: 0.1882 - val_accuracy: 0.9650\n",
      "Epoch 18/40\n",
      "1974/1974 [==============================] - 341s 173ms/step - loss: 0.0331 - accuracy: 0.9936 - val_loss: 0.1234 - val_accuracy: 0.9776\n",
      "Epoch 19/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0335 - accuracy: 0.9936 - val_loss: 0.0700 - val_accuracy: 0.9888\n",
      "Epoch 20/40\n",
      "1974/1974 [==============================] - 337s 170ms/step - loss: 0.0297 - accuracy: 0.9946 - val_loss: 0.2426 - val_accuracy: 0.9534\n",
      "Epoch 21/40\n",
      "1974/1974 [==============================] - 337s 171ms/step - loss: 0.0299 - accuracy: 0.9943 - val_loss: 5.0705 - val_accuracy: 0.5316\n",
      "Epoch 22/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0294 - accuracy: 0.9944 - val_loss: 0.0593 - val_accuracy: 0.9888\n",
      "Epoch 23/40\n",
      "1974/1974 [==============================] - 337s 170ms/step - loss: 0.0334 - accuracy: 0.9943 - val_loss: 0.0257 - val_accuracy: 0.9956\n",
      "Epoch 24/40\n",
      "1974/1974 [==============================] - 337s 171ms/step - loss: 0.0303 - accuracy: 0.9945 - val_loss: 0.0617 - val_accuracy: 0.9886\n",
      "Epoch 25/40\n",
      "1974/1974 [==============================] - 337s 170ms/step - loss: 0.0259 - accuracy: 0.9949 - val_loss: 0.0393 - val_accuracy: 0.9916\n",
      "Epoch 26/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0298 - accuracy: 0.9944 - val_loss: 0.0653 - val_accuracy: 0.9872\n",
      "Epoch 27/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0288 - accuracy: 0.9943 - val_loss: 0.0673 - val_accuracy: 0.9905\n",
      "Epoch 28/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0315 - accuracy: 0.9946 - val_loss: 0.0653 - val_accuracy: 0.9870\n",
      "Epoch 29/40\n",
      "1974/1974 [==============================] - 337s 171ms/step - loss: 0.0254 - accuracy: 0.9952 - val_loss: 0.7537 - val_accuracy: 0.9164\n",
      "Epoch 30/40\n",
      "1974/1974 [==============================] - 337s 171ms/step - loss: 0.0303 - accuracy: 0.9947 - val_loss: 0.1185 - val_accuracy: 0.9791\n",
      "Epoch 31/40\n",
      "1974/1974 [==============================] - 337s 170ms/step - loss: 0.0294 - accuracy: 0.9946 - val_loss: 0.0996 - val_accuracy: 0.9826\n",
      "Epoch 32/40\n",
      "1974/1974 [==============================] - 337s 171ms/step - loss: 0.0292 - accuracy: 0.9950 - val_loss: 0.0851 - val_accuracy: 0.9817\n",
      "Epoch 33/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0260 - accuracy: 0.9954 - val_loss: 0.1533 - val_accuracy: 0.9771\n",
      "Epoch 34/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0249 - accuracy: 0.9955 - val_loss: 0.2466 - val_accuracy: 0.9606\n",
      "Epoch 35/40\n",
      "1974/1974 [==============================] - 827s 419ms/step - loss: 0.0288 - accuracy: 0.9949 - val_loss: 0.1983 - val_accuracy: 0.9690\n",
      "Epoch 36/40\n",
      "1974/1974 [==============================] - 339s 172ms/step - loss: 0.0248 - accuracy: 0.9956 - val_loss: 0.0316 - val_accuracy: 0.9934\n",
      "Epoch 37/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0247 - accuracy: 0.9954 - val_loss: 0.1356 - val_accuracy: 0.9798\n",
      "Epoch 38/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0274 - accuracy: 0.9953 - val_loss: 0.0640 - val_accuracy: 0.9927\n",
      "Epoch 39/40\n",
      "1974/1974 [==============================] - 336s 170ms/step - loss: 0.0255 - accuracy: 0.9959 - val_loss: 0.0632 - val_accuracy: 0.9908\n",
      "Epoch 40/40\n",
      "1974/1974 [==============================] - 337s 171ms/step - loss: 0.0262 - accuracy: 0.9955 - val_loss: 0.2914 - val_accuracy: 0.9694\n"
     ]
    }
   ],
   "source": [
    "history_resnet50=resnet50_model.fit(train_generator, epochs=40, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the history of mobilenet    \n",
    "save_history_resnet50 = pd.DataFrame(history_resnet50.history)\n",
    "\n",
    "save_history_resnet50.to_csv(\"resnet50_model_training_history_40epoch_1211.csv\", index=False)\n",
    "\n",
    "save_path=r'resnet50_model1213_40epochs.h5'\n",
    "resnet50_model.save(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
